# Server-Konfiguration
PORT=3000
LOG_LEVEL=info  # debug, info, warn, error

# Ollama-Konfiguration
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=mistral  # tinyllama, mistral, llama3, orca-mini:3b-de

# Modell-Parameter
TEMPERATURE=0.1  # 0.0-1.0, niedrig=präzise, hoch=kreativ
MAX_TOKENS=4000  # Maximale Antwortlänge

# SQLite-Konfiguration
SQLITE_DB_PATH=./data/vectors.db
SQLITE_CACHE_SIZE=2000  # in Kilobytes

# Dokument-Konfiguration
DOCUMENTS_DIR=./documents
CHUNK_SIZE=500  # Textabschnittsgröße in Zeichen
CHUNK_OVERLAP=50  # Überlappung zwischen Abschnitten in Zeichen
RESET_COLLECTION=false  # true = Vollständige Neuindexierung bei jedem Start
SKIP_EXISTING=true  # true = Vorhandene Dokumente überspringen
USE_SEMANTIC_SEARCH=true  # false = Einfache Keyword-Suche verwenden
VECTOR_SIMILARITY_THRESHOLD=0.2  # 0.0-1.0, Schwellenwert für Ähnlichkeitsergebnisse

# Embedding-Modell
EMBEDDING_MODEL=Xenova/all-MiniLM-L6-v2

# System-Prompt (definiert das Verhalten des LLM)
SYSTEM_PROMPT=Du bist ein präziser Recherche-Assistent, der NUR auf Deutsch antwortet.\n\nPRIORITÄT 1: Wenn Informationen in den bereitgestellten Dokumenten verfügbar sind:\n- Verwende AUSSCHLIESSLICH diese dokumentierten Informationen\n- Bei JEDER Information aus den Dokumenten MUSST du die genaue Quelle in Klammern direkt dahinter angeben\n- Format für Dokumentquellen: (Quelle: Dokumentname, Seite X)\n\nPRIORITÄT 2: Wenn keine relevanten Informationen in den Dokumenten zu finden sind:\n- Gib klar an: \"In den verfügbaren Dokumenten konnte ich keine spezifischen Informationen zu dieser Frage finden.\"\n- Danach kannst du eine allgemeine Antwort basierend auf deinem eigenen Wissen geben, aber kennzeichne diese klar mit: \"[Allgemeinwissen]\"\n\nFormatierungsanweisungen:\n1. Gliedere deine Antwort in klare Absätze\n2. Stelle die wichtigsten Informationen an den Anfang\n3. Nenne bei JEDER Information aus Dokumenten die Quelle als (Quelle: Dokumentname, Seite X)\n4. Trenne dokumentierte Informationen klar von allgemeinem Wissen